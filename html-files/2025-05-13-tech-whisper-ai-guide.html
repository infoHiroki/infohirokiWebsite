<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Whisper AI 実装ガイド</title>
    <style>
        body {
            font-family: 'Helvetica Neue', Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
        }
        header {
            text-align: center;
            margin-bottom: 30px;
        }
        h1 {
            color: #4285f4;
            font-size: 28px;
            margin-bottom: 10px;
        }
        h2 {
            color: #4285f4;
            font-size: 22px;
            margin-top: 30px;
            border-bottom: 2px solid #4285f4;
            padding-bottom: 8px;
        }
        .key-points {
            background-color: #f5f9ff;
            border-left: 4px solid #4285f4;
            padding: 15px 20px;
            border-radius: 0 5px 5px 0;
            margin: 25px 0;
        }
        ul {
            padding-left: 20px;
        }
        ul ul {
            padding-left: 30px;
        }
        ul ul ul {
            padding-left: 40px;
        }
        .step {
            position: relative;
            margin-bottom: 25px;
            padding-left: 40px;
        }
        .step-number {
            position: absolute;
            left: 0;
            top: 0;
            width: 30px;
            height: 30px;
            background-color: #4285f4;
            color: white;
            border-radius: 50%;
            text-align: center;
            line-height: 30px;
            font-weight: bold;
        }
        .code-block {
            background-color: #f5f9ff;
            padding: 15px;
            border-radius: 5px;
            border-left: 4px solid #4285f4;
            font-family: monospace;
            white-space: pre-wrap;
            margin: 20px 0;
        }
        .summary {
            background-color: #f5f9ff;
            padding: 20px;
            border-radius: 5px;
            margin-top: 30px;
            border: 1px solid #4285f4;
        }
        .summary h3 {
            color: #4285f4;
            margin-top: 0;
        }
        .note {
            background-color: #fffbf5;
            border-left: 4px solid #fbbc04;
            padding: 10px 15px;
            margin: 15px 0;
            font-size: 0.9em;
        }
    </style>
</head>
<body>
    <header>
        <h1>Whisper AI 実装ガイド</h1>
        <p>Pythonを使った音声認識AIの導入と活用方法</p>
    </header>

    <div class="key-points">
        <h2>重要ポイント</h2>
        <ul>
            <li>Whisper AIの基本情報
                <ul>
                    <li>OpenAIが公開している音声認識AI</li>
                    <li>音声をテキストに変換する機能を提供</li>
                    <li>WebAPIとローカル環境の両方で利用可能</li>
                    <li>ローカルで使用する場合は無料で利用可能</li>
                    <li>オフライン環境でも動作可能（モデルのダウンロード後）</li>
                </ul>
            </li>
            <li>環境構築手順
                <ul>
                    <li>Pythonライブラリのインストール
                        <ul>
                            <li>pip install openai-whisper</li>
                        </ul>
                    </li>
                    <li>ffmpegのインストール
                        <ul>
                            <li>Mac: brew install ffmpeg</li>
                            <li>Linux/Ubuntu: apt install ffmpeg</li>
                            <li>Windows: choco install ffmpeg（Chocolateyが必要）</li>
                        </ul>
                    </li>
                </ul>
            </li>
            <li>Pythonでの実装方法
                <ul>
                    <li>モデルのロード
                        <ul>
                            <li>サイズ選択: base, small, medium, large, large-v2, large-v3</li>
                            <li>大きいモデルほど精度が高い</li>
                            <li>すべてのモデルで日本語対応</li>
                        </ul>
                    </li>
                    <li>音声ファイルの変換処理
                        <ul>
                            <li>transcribe()メソッドを使用</li>
                            <li>結果は辞書型で返却（'text'キーに変換テキスト、'language'キーに言語情報）</li>
                        </ul>
                    </li>
                </ul>
            </li>
            <li>GPU活用による高速化
                <ul>
                    <li>長い音声ファイルや高精度モデルでは処理時間が長くなる</li>
                    <li>GPUを使用することで処理速度を向上可能</li>
                    <li>Google Cloud VMインスタンス等でGPU環境を構築可能</li>
                    <li>Google CloudでのGPUリソース申請手順
                        <ul>
                            <li>ナビゲーションメニューの「IAMと管理」から「割り当て」を選択</li>
                            <li>「GPUs (all regions)」でフィルター</li>
                            <li>サジェストされた「GPUs (all regions)」を選択</li>
                            <li>チェックボックスにチェックを入れて「割り当てを編集」をクリック</li>
                            <li>必要な項目を入力して「リクエストを送信」をクリック</li>
                        </ul>
                    </li>
                    <li>PyTorchを利用してGPUを指定する実装が必要</li>
                </ul>
            </li>
        </ul>
    </div>

    <h2>基本的な実装手順</h2>
    <div class="step">
        <div class="step-number">1</div>
        <p><strong>必要なライブラリのインストール</strong><br>
        Whisperライブラリとffmpegをインストールします。</p>
        <div class="code-block">pip install openai-whisper
# OSによって異なるffmpegのインストール方法
# Mac: brew install ffmpeg
# Linux: sudo apt install ffmpeg
# Windows: choco install ffmpeg（Chocolateyが必要）
# Chocolateyのインストール: https://chocolatey.org/install</div>
        <div class="note">
            <p><strong>Windowsユーザー向け:</strong> Chocolateyのインストールが必要です。公式サイト（<a href="https://chocolatey.org/install" target="_blank">https://chocolatey.org/install</a>）の手順に従ってインストールしてください。</p>
        </div>
    </div>

    <div class="step">
        <div class="step-number">2</div>
        <p><strong>Pythonスクリプトの作成</strong><br>
        Whisperをインポートし、モデルをロードします。</p>
        <div class="code-block">import whisper

# モデルのロード（初回はダウンロードが行われる）
model = whisper.load_model("base")  # base, small, medium, large, large-v2, large-v3</div>
    </div>

    <div class="step">
        <div class="step-number">3</div>
        <p><strong>音声ファイルの変換処理</strong><br>
        音声ファイルをテキストに変換し、結果を取得します。</p>
        <div class="code-block"># 音声をテキストに変換
result = model.transcribe("test.wav")

# 結果の表示
print(result["text"])
print(result["language"])  # 言語の表示（日本語の場合は'ja'）</div>
    </div>

    <div class="note">
        <p><strong>注意点:</strong> 初回実行時にはモデルのダウンロードが行われるため、時間がかかる場合があります。また、精度は使用するモデルのサイズによって異なります。高精度が必要な場合は、より大きなモデル（largeなど）を選択してください。</p>
    </div>

    <h2>GPU活用による高速化</h2>
    <div class="step">
        <div class="step-number">4</div>
        <p><strong>Google CloudでのGPUリソース申請</strong><br>
        Google CloudでGPUを使用するには、まず割り当て申請が必要です。</p>
        <ol style="background-color: #f5f9ff; padding: 15px 20px; border-radius: 5px; border-left: 4px solid #4285f4;">
            <li>ナビゲーションメニューの「IAMと管理」から「割り当て」を選択</li>
            <li>「GPUs (all regions)」でフィルター</li>
            <li>サジェストされた「GPUs (all regions)」を選択</li>
            <li>チェックボックスにチェックを入れて「割り当てを編集」をクリック</li>
            <li>必要な項目を入力して「リクエストを送信」をクリック</li>
        </ol>
    </div>

    <div class="step">
        <div class="step-number">5</div>
        <p><strong>GPUを使用するための実装</strong><br>
        PyTorchを使ってGPUを活用する場合の実装例です。</p>
        <div class="code-block">import whisper
import torch

# GPUが利用可能か確認
device = "cuda" if torch.cuda.is_available() else "cpu"

# モデルのロード（GPUを指定）
model = whisper.load_model("base", device=device)

# 音声をテキストに変換
result = model.transcribe("test.wav")

# 結果の表示
print(result["text"])</div>
    </div>

    <div class="note">
        <p><strong>GPU使用状況の確認方法:</strong> NVIDIAのGPUを使用している場合は、<code>nvidia-smi</code>コマンドを実行することでGPUの使用状況を確認できます。</p>
    </div>

    <div class="summary">
        <h3>要約</h3>
        <p>Whisper AIはOpenAIが開発した高性能な音声認識AIで、音声をテキストに変換する機能を提供しています。WebAPIとしての利用だけでなく、ローカル環境でも無料で使用できるため、インターネット接続がない環境でも活用できる利点があります。</p>
        
        <p>実装にはPythonを使い、必要なライブラリ（openai-whisper）とffmpegをインストールするだけで簡単に始められます。モデルサイズは複数用意されており、baseからlarge-v3まで選択可能で、大きいモデルほど高精度な認識が可能です。すべてのモデルで日本語の音声認識に対応しています。</p>
        
        <p>具体的な実装では、whisperモジュールをインポートし、load_model()でAIモデルをロード、transcribe()メソッドで音声ファイルをテキスト変換します。結果は辞書型で返され、テキスト内容や検出された言語情報を取得できます。長い音声ファイルや高精度モデルを使用する場合は処理時間が長くなるため、GPUを活用することで高速化が可能です。Google CloudのVMインスタンスなどを利用してGPU環境を構築し、PyTorchを使ってGPUを指定する実装を行うことで、より効率的な音声認識処理が実現できます。</p>
    </div>
</body>
</html>